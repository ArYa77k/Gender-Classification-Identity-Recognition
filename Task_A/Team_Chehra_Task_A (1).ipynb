{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rxG0m_Tkb4Dp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d86727cc"
      },
      "source": [
        "# Chehra: FACECOM Face Analysis in Challenging Environments (COMSYS 5 Hackathon Submission) - Task A: Gender Classification\n",
        "\n",
        "Welcome to \"**Chehra**,\" our submission for the **COMSYS 5 FACECOM Hackathon**. This notebook focuses *primarily* on **Task A: Gender Classification (Binary Classification)**, addressing the challenge of predicting gender from face images under various visual degradations such as motion blur, fog, rain, low light, and uneven lighting.\n",
        "\n",
        "The overall hackathon includes two main tasks:\n",
        "\n",
        "- **Task A: Gender Classification (Binary Classification)**\n",
        "- **Task B: Face Recognition (Multi-class Classification)**\n",
        "\n",
        "The dataset used is **FACECOM (Face Attributes in Challenging Environments)**, specifically designed for this challenge with diverse visual conditions and annotations for Gender and Person Identity.\n",
        "\n",
        "This notebook is organized to present our solutions for Task A, including training, validation, and ensemble inference. While Task B is part of the hackathon, this notebook focuses on Task A.\n",
        "\n",
        "Let's dive into the details of our approach for Task A."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task A: Gender Classification (Binary Classification)\n",
        "\n",
        "This section focuses on **Task A**: **Gender Classification**. The goal is to build a robust model that can accurately predict the gender (Male/Female) from face images, even those affected by various visual degradations.\n",
        "\n",
        "We are using a **ResNet18** model, pre-trained on ImageNet, and fine-tuning it on the provided Task A training dataset. ResNet models are chosen for their balance of performance and computational efficiency, as well as their effectiveness in handling edge cases and distortions, making them suitable for this challenging dataset.\n",
        "\n",
        "Key techniques used in the training process include:\n",
        "- **Albumentations** for extensive data augmentation to improve robustness to variations in the data, including various distortions like blur, fog, noise, and rain.\n",
        "- **Weighted Random Sampler** to address class imbalance in the training data by oversampling the minority class.\n",
        "- **Class Weights** in the loss function to further mitigate the impact of class imbalance.\n",
        "\n",
        "The model's performance is evaluated using **Accuracy**, **Precision**, **Recall**, and **F1-Score** on the validation set."
      ],
      "metadata": {
        "id": "rxG0m_Tkb4Dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script contains the core training logic for **Task A: Gender Classification**. It utilizes a ResNet18 model, applies extensive data augmentation using Albumentations, handles class imbalance with Weighted Random Sampler and class weights in the loss function, and trains the model for a specified number of epochs, saving the best model based on training F1-Score."
      ],
      "metadata": {
        "id": "tfa3Iux9cEwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torchvision import models\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# ----------------------- Dataset Class ----------------------- #\n",
        "class TaskAGenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.label_map = {\"male\": 0, \"female\": 1}\n",
        "        self.label_counts = {0: 0, 1: 0}  # Track counts for weighting\n",
        "\n",
        "        for label_name in [\"male\", \"female\"]:\n",
        "            folder = root_dir / label_name\n",
        "            if not folder.exists():\n",
        "                print(f\"⚠️ Folder not found: {folder}\")\n",
        "                continue\n",
        "\n",
        "            for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"]:\n",
        "                for img_path in folder.glob(ext):\n",
        "                    label = self.label_map[label_name]\n",
        "                    self.samples.append((img_path, label))\n",
        "                    self.label_counts[label] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, label\n",
        "\n",
        "# ----------------------- Training Function ----------------------- #\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
        "    return total_loss / len(loader.dataset), acc, precision, recall, f1\n",
        "\n",
        "# ----------------------- Reproducibility ----------------------- #\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ----------------------- Main Function ----------------------- #\n",
        "def main(args):\n",
        "    set_seed()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
        "            A.MotionBlur(blur_limit=5, p=0.5),\n",
        "        ], p=0.3),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.6, alpha_coef=0.1, p=1.0),\n",
        "            A.RandomBrightnessContrast(brightness_limit=(-0.6, -0.3), contrast_limit=0.2, p=1.0),\n",
        "            A.RandomBrightnessContrast(brightness_limit=(0.5, 0.9), contrast_limit=0.2, p=1.0),\n",
        "        ], p=0.4),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(var_limit=(20.0, 60.0), p=0.5),\n",
        "            A.ISONoise(intensity=(0.2, 0.5), p=0.5),\n",
        "            A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.5),\n",
        "        ], p=0.3),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.Downscale(scale_min=0.3, scale_max=0.7, interpolation=1, p=1.0),\n",
        "            A.ImageCompression(quality_lower=10, quality_upper=30, p=1.0),\n",
        "        ], p=0.3),\n",
        "\n",
        "        A.RandomRain(blur_value=2, brightness_coefficient=0.9, drop_color=(200, 200, 200), p=0.3),\n",
        "\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    train_data = TaskAGenderDataset(Path(args.train_path), transform=train_transform)\n",
        "\n",
        "    # --- Compute class weights --- #\n",
        "    total = train_data.label_counts[0] + train_data.label_counts[1]\n",
        "    male_weight = total / (2 * train_data.label_counts[0])\n",
        "    female_weight = total / (2 * train_data.label_counts[1])\n",
        "    class_weights = torch.tensor([male_weight, female_weight], dtype=torch.float32).to(device)\n",
        "    print(f\"Class weights: {class_weights.tolist()}\")\n",
        "\n",
        "    # Optional: Oversample females\n",
        "    targets = [label for _, label in train_data.samples]\n",
        "    class_sample_count = np.bincount(targets)\n",
        "    weights = 1. / class_sample_count[targets]\n",
        "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size, sampler=sampler)\n",
        "\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "\n",
        "        train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(\n",
        "            model, train_loader, optimizer, criterion, device\n",
        "        )\n",
        "\n",
        "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
        "\n",
        "        if train_f1 > best_f1:\n",
        "            best_f1 = train_f1\n",
        "            torch.save(model.state_dict(), args.save_path)\n",
        "            full_model_path = args.save_path.replace('.pth', '_full.pt')\n",
        "            torch.save(model, full_model_path)\n",
        "            print(f\"\\n✅ Best model saved with F1: {best_f1:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    class Args:\n",
        "        train_path = \"/kaggle/input/comsys/Comys_Hackathon5/Task_A/train\"\n",
        "        batch_size = 64\n",
        "        epochs = 25\n",
        "        lr = 1e-4\n",
        "        save_path = \"best_gender_model.pth\"\n",
        "\n",
        "    args = Args()\n",
        "\n",
        "    temp_dataset = TaskAGenderDataset(Path(args.train_path))\n",
        "    if len(temp_dataset) == 0:\n",
        "        raise ValueError(f\"No images found in {args.train_path}. Check if 'male' and 'female' folders contain images.\")\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "id": "zDO4XEXgdEwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/tmp/ipykernel_35/3613758861.py:96: UserWarning: Argument(s) 'fog_coef_lower, fog_coef_upper' are not valid for transform RandomFog\n",
        "  A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.6, alpha_coef=0.1, p=1.0),\n",
        "/tmp/ipykernel_35/3613758861.py:102: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
        "  A.GaussNoise(var_limit=(20.0, 60.0), p=0.5),\n",
        "/tmp/ipykernel_35/3613758861.py:108: UserWarning: Argument(s) 'scale_min, scale_max, interpolation' are not valid for transform Downscale\n",
        "  A.Downscale(scale_min=0.3, scale_max=0.7, interpolation=1, p=1.0),\n",
        "/tmp/ipykernel_35/3613758861.py:109: UserWarning: Argument(s) 'quality_lower, quality_upper' are not valid for transform ImageCompression\n",
        "  A.ImageCompression(quality_lower=10, quality_upper=30, p=1.0),\n",
        "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
        "  warnings.warn(msg)\n",
        "Class weights: [0.6285901069641113, 2.444162368774414]\n",
        "\n",
        "Epoch 1/25\n",
        "Training: 100%|██████████| 31/31 [00:27<00:00,  1.14it/s]\n",
        "Train - Loss: 0.3338, Acc: 0.7342, Prec: 0.6681, Rec: 0.9669, F1: 0.7902\n",
        "\n",
        "✅ Best model saved with F1: 0.7902\n",
        "\n",
        "Epoch 2/25\n",
        "Training: 100%|██████████| 31/31 [00:30<00:00,  1.03it/s]\n",
        "Train - Loss: 0.2520, Acc: 0.7996, Prec: 0.7237, Rec: 0.9753, F1: 0.8309\n",
        "\n",
        "✅ Best model saved with F1: 0.8309\n",
        "\n",
        "Epoch 3/25\n",
        "Training: 100%|██████████| 31/31 [00:31<00:00,  1.02s/it]\n",
        "Train - Loss: 0.2118, Acc: 0.8401, Prec: 0.7654, Rec: 0.9634, F1: 0.8531\n",
        "\n",
        "✅ Best model saved with F1: 0.8531\n",
        "\n",
        "Epoch 4/25\n",
        "Training: 100%|██████████| 31/31 [00:29<00:00,  1.05it/s]\n",
        "Train - Loss: 0.1716, Acc: 0.8671, Prec: 0.7982, Rec: 0.9877, F1: 0.8829\n",
        "\n",
        "✅ Best model saved with F1: 0.8829\n",
        "\n",
        "Epoch 5/25\n",
        "Training: 100%|██████████| 31/31 [00:28<00:00,  1.10it/s]\n",
        "Train - Loss: 0.1516, Acc: 0.8884, Prec: 0.8360, Rec: 0.9668, F1: 0.8967\n",
        "\n",
        "✅ Best model saved with F1: 0.8967\n",
        "\n",
        "Epoch 6/25\n",
        "Training: 100%|██████████| 31/31 [00:28<00:00,  1.08it/s]\n",
        "Train - Loss: 0.1337, Acc: 0.9003, Prec: 0.8477, Rec: 0.9773, F1: 0.9079\n",
        "\n",
        "✅ Best model saved with F1: 0.9079\n",
        "\n",
        "Epoch 7/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.18it/s]\n",
        "Train - Loss: 0.1295, Acc: 0.9045, Prec: 0.8506, Rec: 0.9824, F1: 0.9118\n",
        "\n",
        "✅ Best model saved with F1: 0.9118\n",
        "\n",
        "Epoch 8/25\n",
        "Training: 100%|██████████| 31/31 [00:27<00:00,  1.11it/s]\n",
        "Train - Loss: 0.1415, Acc: 0.8956, Prec: 0.8432, Rec: 0.9765, F1: 0.9050\n",
        "\n",
        "Epoch 9/25\n",
        "Training: 100%|██████████| 31/31 [00:25<00:00,  1.22it/s]\n",
        "Train - Loss: 0.1356, Acc: 0.8941, Prec: 0.8288, Rec: 0.9816, F1: 0.8987\n",
        "\n",
        "Epoch 10/25\n",
        "Training: 100%|██████████| 31/31 [00:28<00:00,  1.10it/s]\n",
        "Train - Loss: 0.0973, Acc: 0.9351, Prec: 0.8971, Rec: 0.9869, F1: 0.9398\n",
        "\n",
        "✅ Best model saved with F1: 0.9398\n",
        "\n",
        "Epoch 11/25\n",
        "Training: 100%|██████████| 31/31 [00:25<00:00,  1.19it/s]\n",
        "Train - Loss: 0.1363, Acc: 0.9024, Prec: 0.8501, Rec: 0.9713, F1: 0.9067\n",
        "\n",
        "Epoch 12/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.16it/s]\n",
        "Train - Loss: 0.1007, Acc: 0.9268, Prec: 0.8829, Rec: 0.9845, F1: 0.9309\n",
        "\n",
        "Epoch 13/25\n",
        "Training: 100%|██████████| 31/31 [00:25<00:00,  1.20it/s]\n",
        "Train - Loss: 0.0981, Acc: 0.9299, Prec: 0.8858, Rec: 0.9898, F1: 0.9349\n",
        "\n",
        "Epoch 14/25\n",
        "Training: 100%|██████████| 31/31 [00:27<00:00,  1.12it/s]\n",
        "Train - Loss: 0.1033, Acc: 0.9247, Prec: 0.8749, Rec: 0.9884, F1: 0.9282\n",
        "\n",
        "Epoch 15/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.17it/s]\n",
        "Train - Loss: 0.0797, Acc: 0.9429, Prec: 0.9007, Rec: 0.9948, F1: 0.9454\n",
        "\n",
        "✅ Best model saved with F1: 0.9454\n",
        "\n",
        "Epoch 16/25\n",
        "Training: 100%|██████████| 31/31 [00:25<00:00,  1.20it/s]\n",
        "Train - Loss: 0.0755, Acc: 0.9496, Prec: 0.9159, Rec: 0.9896, F1: 0.9513\n",
        "\n",
        "✅ Best model saved with F1: 0.9513\n",
        "\n",
        "Epoch 17/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.17it/s]\n",
        "Train - Loss: 0.0685, Acc: 0.9465, Prec: 0.9129, Rec: 0.9909, F1: 0.9503\n",
        "\n",
        "Epoch 18/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.16it/s]\n",
        "Train - Loss: 0.0817, Acc: 0.9507, Prec: 0.9199, Rec: 0.9876, F1: 0.9525\n",
        "\n",
        "✅ Best model saved with F1: 0.9525\n",
        "\n",
        "Epoch 19/25\n",
        "Training: 100%|██████████| 31/31 [00:25<00:00,  1.22it/s]\n",
        "Train - Loss: 0.0723, Acc: 0.9486, Prec: 0.9153, Rec: 0.9862, F1: 0.9494\n",
        "\n",
        "Epoch 20/25\n",
        "Training: 100%|██████████| 31/31 [00:25<00:00,  1.20it/s]\n",
        "Train - Loss: 0.0687, Acc: 0.9491, Prec: 0.9116, Rec: 0.9948, F1: 0.9514\n",
        "\n",
        "Epoch 21/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.19it/s]\n",
        "Train - Loss: 0.0773, Acc: 0.9444, Prec: 0.9128, Rec: 0.9858, F1: 0.9479\n",
        "\n",
        "Epoch 22/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.17it/s]\n",
        "Train - Loss: 0.0678, Acc: 0.9569, Prec: 0.9311, Rec: 0.9900, F1: 0.9596\n",
        "\n",
        "✅ Best model saved with F1: 0.9596\n",
        "\n",
        "Epoch 23/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.18it/s]\n",
        "Train - Loss: 0.0716, Acc: 0.9512, Prec: 0.9185, Rec: 0.9896, F1: 0.9527\n",
        "\n",
        "Epoch 24/25\n",
        "Training: 100%|██████████| 31/31 [00:27<00:00,  1.11it/s]\n",
        "Train - Loss: 0.0594, Acc: 0.9553, Prec: 0.9237, Rec: 0.9927, F1: 0.9570\n",
        "\n",
        "Epoch 25/25\n",
        "Training: 100%|██████████| 31/31 [00:26<00:00,  1.16it/s]\n",
        "Train - Loss: 0.0773, Acc: 0.9502, Prec: 0.9212, Rec: 0.9856, F1: 0.9523\n"
      ],
      "metadata": {
        "id": "xcxF4reVdNED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script performs inference and evaluation of the trained Gender Classification model (Task A) on the official validation set (`/kaggle/input/comsys-hack-dataset/Comys_Hackathon5/Task_A/val`). It loads the best saved model, runs predictions on the validation data, and prints a detailed classification report including Accuracy, Precision, Recall, and F1-Score. It also saves the predictions to a CSV file."
      ],
      "metadata": {
        "id": "JqWRWq--dWul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ----------- Dataset for Inference ----------- #\n",
        "class GenderValDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.label_map = {\"male\": 0, \"female\": 1}\n",
        "\n",
        "        for label_name in [\"male\", \"female\"]:\n",
        "            folder = root_dir / label_name\n",
        "            for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"]:\n",
        "                for img_path in folder.glob(ext):\n",
        "                    self.samples.append((img_path, self.label_map[label_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label, path.name  # also return filename\n",
        "\n",
        "# ----------- Transforms ----------- #\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ----------- Load Model ----------- #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet18()\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "model.load_state_dict(torch.load(\"/kaggle/input/model-and-imagestst/best_gender_model best one.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ----------- Load Validation Dataset ----------- #\n",
        "val_dir = Path(\"/kaggle/input/comsys-hack-dataset/Comys_Hackathon5/Task_A/val\")\n",
        "val_dataset = GenderValDataset(val_dir, transform=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ----------- Inference ----------- #\n",
        "all_preds, all_labels, filenames = [], [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels, names in tqdm(val_loader, desc=\"Running Inference\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "        filenames.extend(names)\n",
        "\n",
        "# ----------- Report ----------- #\n",
        "print(\"\\n📊 Classification Report on Validation Set:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"male\", \"female\"]))\n",
        "\n",
        "# ----------- Optional: Save predictions to CSV ----------- #\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    \"filename\": filenames,\n",
        "    \"true_label\": all_labels,\n",
        "    \"predicted_label\": all_preds\n",
        "})\n",
        "df.to_csv(\"gender_predictions_val.csv\", index=False)\n",
        "print(\"\\n✅ Predictions saved to gender_predictions_val.csv\")\n"
      ],
      "metadata": {
        "id": "haD8iVZSe1xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script provides a custom inference function for the trained Gender Classification model (Task A) allowing predictions on a specified folder of images. It loads a saved model, preprocesses images, performs inference, and can optionally save predictions to a CSV and display a grid of images with their predicted labels and confidence scores."
      ],
      "metadata": {
        "id": "MNiwh-iHe7DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import models, transforms\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# ✅ Image Preprocessing (same as training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ Label Map\n",
        "idx2label = {0: \"male\", 1: \"female\"}\n",
        "\n",
        "# ✅ Inference Function\n",
        "def run_inference(model_path, image_folder, show_visuals=True, save_csv=True):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load model\n",
        "    model = models.resnet18(pretrained=False)\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    image_folder = Path(image_folder)\n",
        "    image_paths = sorted([p for p in image_folder.glob(\"*\") if p.suffix.lower() in ['.jpg', '.jpeg', '.png']])\n",
        "\n",
        "    if len(image_paths) == 0:\n",
        "        raise ValueError(f\"No image files found in {image_folder}\")\n",
        "\n",
        "    predictions = []\n",
        "    images_for_display = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_path in image_paths:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(input_tensor)\n",
        "            probs = F.softmax(output, dim=1)\n",
        "            pred_label = torch.argmax(probs, dim=1).item()\n",
        "            pred_conf = probs[0][pred_label].item()\n",
        "\n",
        "            predictions.append({\n",
        "                \"filename\": img_path.name,\n",
        "                \"predicted_label\": idx2label[pred_label],\n",
        "                \"confidence\": round(pred_conf, 4)\n",
        "            })\n",
        "\n",
        "            # Save for visualization\n",
        "            images_for_display.append((image.resize((128, 128)), idx2label[pred_label], round(pred_conf, 2)))\n",
        "\n",
        "    # ✅ Save Predictions CSV\n",
        "    df = pd.DataFrame(predictions)\n",
        "    if save_csv:\n",
        "        df.to_csv(\"custom_predictions.csv\", index=False)\n",
        "        print(\"✅ Predictions saved to custom_predictions.csv\")\n",
        "\n",
        "    # ✅ Display Sample Grid\n",
        "    if show_visuals:\n",
        "        show_predictions_grid(images_for_display)\n",
        "\n",
        "    return df\n",
        "\n",
        "# ✅ Display Grid of Predictions\n",
        "def show_predictions_grid(images_with_labels):\n",
        "    num_images = len(images_with_labels)\n",
        "    cols = 5\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "    plt.figure(figsize=(15, 3 * rows))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img, label, conf = images_with_labels[i]\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"{label} ({conf*100:.1f}%)\", fontsize=10)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ✅ Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Change these paths:\n",
        "    model_path = \"/kaggle/input/model-and-imagestst/best_gender_model best one.pth\"\n",
        "    test_images_path = \"/kaggle/input/model-and-imagestst\"\n",
        "\n",
        "    run_inference(model_path, test_images_path)\n"
      ],
      "metadata": {
        "id": "306E4AcrfBvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cell contains a training script for a second ResNet18 model for Task A (Gender Classification). Similar to the first training script, it includes the `TaskAGenderDataset` class, `train_one_epoch` function, reproducibility settings, and the main training loop. It utilizes Albumentations for data augmentation, WeightedRandomSampler and class weights for handling class imbalance, and saves the best model based on training F1-Score."
      ],
      "metadata": {
        "id": "xXpEzR_Tqmm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#task A second model running code\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torchvision import models\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# ----------------------- Dataset Class ----------------------- #\n",
        "class TaskAGenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.label_map = {\"male\": 0, \"female\": 1}\n",
        "        self.label_counts = {0: 0, 1: 0}\n",
        "\n",
        "        for label_name in [\"male\", \"female\"]:\n",
        "            folder = root_dir / label_name\n",
        "            if not folder.exists():\n",
        "                print(f\"⚠️ Folder not found: {folder}\")\n",
        "                continue\n",
        "\n",
        "            for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"]:\n",
        "                for img_path in folder.glob(ext):\n",
        "                    label = self.label_map[label_name]\n",
        "                    self.samples.append((img_path, label))\n",
        "                    self.label_counts[label] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, label\n",
        "\n",
        "# ----------------------- Training Function ----------------------- #\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
        "    return total_loss / len(loader.dataset), acc, precision, recall, f1\n",
        "\n",
        "# ----------------------- Reproducibility ----------------------- #\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ----------------------- Main Function ----------------------- #\n",
        "def main(args):\n",
        "    set_seed()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0), ratio=(0.75, 1.33)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=15, p=0.4),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
        "            A.MotionBlur(blur_limit=5, p=0.5),\n",
        "        ], p=0.3),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.6, alpha_coef=0.1),\n",
        "            A.RandomBrightnessContrast(brightness_limit=(-0.6, -0.3), contrast_limit=0.2),\n",
        "            A.RandomBrightnessContrast(brightness_limit=(0.5, 0.9), contrast_limit=0.2),\n",
        "        ], p=0.4),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(var_limit=(20.0, 60.0), p=0.5),\n",
        "            A.ISONoise(intensity=(0.2, 0.5), p=0.5),\n",
        "            A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.5),\n",
        "        ], p=0.3),\n",
        "\n",
        "        A.OneOf([\n",
        "            A.Downscale(scale_min=0.3, scale_max=0.7, interpolation=1),\n",
        "            A.ImageCompression(quality_lower=10, quality_upper=30),\n",
        "        ], p=0.3),\n",
        "\n",
        "        A.RandomRain(blur_value=2, brightness_coefficient=0.9, drop_color=(200, 200, 200), p=0.3),\n",
        "\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    train_data = TaskAGenderDataset(Path(args.train_path), transform=train_transform)\n",
        "\n",
        "    # --- Compute class weights --- #\n",
        "    total = train_data.label_counts[0] + train_data.label_counts[1]\n",
        "    male_weight = total / (2 * train_data.label_counts[0])\n",
        "    female_weight = total / (2 * train_data.label_counts[1])\n",
        "    class_weights = torch.tensor([male_weight, female_weight], dtype=torch.float32).to(device)\n",
        "    print(f\"Class weights: {class_weights.tolist()}\")\n",
        "\n",
        "    # Optional: Oversample minority\n",
        "    targets = [label for _, label in train_data.samples]\n",
        "    class_sample_count = np.bincount(targets)\n",
        "    weights = 1. / class_sample_count[targets]\n",
        "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size, sampler=sampler)\n",
        "\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "\n",
        "        train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(\n",
        "            model, train_loader, optimizer, criterion, device\n",
        "        )\n",
        "\n",
        "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
        "\n",
        "        if train_f1 > best_f1:\n",
        "            best_f1 = train_f1\n",
        "            torch.save(model.state_dict(), args.save_path)\n",
        "            torch.save(model, args.save_path.replace('.pth', '_full.pt'))\n",
        "            print(f\"✅ Best model saved with F1: {best_f1:.4f}\")\n",
        "\n",
        "# ----------------------- Args ----------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    class Args:\n",
        "        train_path = \"/kaggle/input/comsys/Comys_Hackathon5/Task_A/train\"\n",
        "        batch_size = 64\n",
        "        epochs = 25\n",
        "        lr = 1e-4\n",
        "        save_path = \"best_gender_model.pth\"\n",
        "\n",
        "    args = Args()\n",
        "\n",
        "    temp_dataset = TaskAGenderDataset(Path(args.train_path))\n",
        "    if len(temp_dataset) == 0:\n",
        "        raise ValueError(f\"No images found in {args.train_path}. Check if 'male' and 'female' folders contain images.\")\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "6tiV9G1unVOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cell provides a validation script for evaluating the second trained Gender Classification model on the validation dataset. It includes label maps, image transforms, a function to load the model state dictionary, and the main evaluation logic that calculates and prints a detailed classification report and displays a confusion matrix."
      ],
      "metadata": {
        "id": "vC7-Vu1wqt3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#task A second model val code\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision import models, transforms\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------- Label Maps ---------------- #\n",
        "idx2label = {0: \"male\", 1: \"female\"}\n",
        "label2idx = {v: k for k, v in idx2label.items()}\n",
        "\n",
        "# ---------------- Transforms ---------------- #\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ---------------- Load Model ---------------- #\n",
        "def load_model(model_path):\n",
        "    model = models.resnet18(pretrained=False)\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    model.eval()\n",
        "    return model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------- Validation Script ---------------- #\n",
        "def evaluate_on_validation(model_path, val_folder):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = load_model(model_path)\n",
        "    val_folder = Path(val_folder)\n",
        "\n",
        "    image_paths = []\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    for label in [\"male\", \"female\"]:\n",
        "        folder = val_folder / label\n",
        "        if not folder.exists():\n",
        "            continue\n",
        "        for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"]:\n",
        "            for img_path in folder.glob(ext):\n",
        "                image_paths.append((img_path, label2idx[label]))\n",
        "\n",
        "    for img_path, label in tqdm(image_paths, desc=\"Evaluating Validation Set\"):\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            pred = torch.argmax(F.softmax(output, dim=1), dim=1).item()\n",
        "\n",
        "        true_labels.append(label)\n",
        "        pred_labels.append(pred)\n",
        "\n",
        "    # --- Show Report --- #\n",
        "    print(\"\\n📊 Classification Report:\")\n",
        "    print(classification_report(true_labels, pred_labels, target_names=[\"male\", \"female\"]))\n",
        "\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"male\", \"female\"])\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(\"Validation Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# ---------------- Entry ---------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"best_gender_model.pth\"  # Change to your model path\n",
        "    val_folder = \"/kaggle/input/comsys/Comys_Hackathon5/Task_A/val\"\n",
        "\n",
        "    evaluate_on_validation(model_path, val_folder)\n"
      ],
      "metadata": {
        "id": "rVLIdwl6nb2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cell implements an ensemble inference strategy for Task A (Gender Classification) by combining predictions from multiple trained ResNet18 models. It uses `facenet_pytorch`'s MTCNN (Multi-task Cascaded Convolutional Networks) for robust face detection and central face cropping with added padding. Each detected and cropped face is preprocessed using standard `torchvision.transforms`. The script loads multiple models from specified paths and performs inference on the preprocessed face images. It calculates weighted average probabilities from the softmax outputs of each model, with weights potentially adjusted dynamically based on the input folder name (e.g., emphasizing male predictions for a \"male\" folder). The final prediction is the class with the highest weighted probability. The script generates a classification report and confusion matrix if ground truth labels are available from the folder structure and can optionally save the predictions with confidence scores to a CSV file and visualize a grid of images with their predictions and confidence levels."
      ],
      "metadata": {
        "id": "IKTZxJKEqHUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision import models, transforms\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "# ---------------- Label Maps ---------------- #\n",
        "idx2label = {0: \"male\", 1: \"female\"}\n",
        "label2idx = {v: k for k, v in idx2label.items()}\n",
        "\n",
        "# ---------------- Transforms ---------------- #\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ---------------- MTCNN Setup ---------------- #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "# ---------------- Face Crop Logic ---------------- #\n",
        "def crop_central_face(image, padding_ratio=0.45):\n",
        "    boxes, probs = mtcnn.detect(image)\n",
        "    if boxes is None or len(boxes) == 0:\n",
        "        return image  # fallback\n",
        "    img_w, img_h = image.size\n",
        "    img_center = np.array([img_w / 2, img_h / 2])\n",
        "    face_centers = np.array([[(x1 + x2) / 2, (y1 + y2) / 2] for (x1, y1, x2, y2) in boxes])\n",
        "    dists = np.linalg.norm(face_centers - img_center, axis=1)\n",
        "    closest_idx = np.argmin(dists)\n",
        "    x1, y1, x2, y2 = boxes[closest_idx]\n",
        "    w, h = x2 - x1, y2 - y1\n",
        "    pad_w, pad_h = w * padding_ratio, h * padding_ratio\n",
        "    x1 = max(int(x1 - pad_w), 0)\n",
        "    y1 = max(int(y1 - pad_h), 0)\n",
        "    x2 = min(int(x2 + pad_w), img_w)\n",
        "    y2 = min(int(y2 + pad_h), img_h)\n",
        "    return image.crop((x1, y1, x2, y2))\n",
        "\n",
        "# ---------------- Load Model ---------------- #\n",
        "def load_model(model_path):\n",
        "    model = models.resnet18(pretrained=False)\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model.to(device)\n",
        "\n",
        "# ---------------- Ensemble Inference ---------------- #\n",
        "def run_ensemble_inference(model_paths, model_weights, image_folder, save_csv=True, show_matrix=True):\n",
        "    assert len(model_paths) == len(model_weights), \"Mismatch in model paths and weights\"\n",
        "    models_list = [load_model(path) for path in model_paths]\n",
        "    image_folder = Path(image_folder)\n",
        "\n",
        "    # Get clean folder name only\n",
        "    folder_name = image_folder.name.lower()\n",
        "    if folder_name == \"male\":\n",
        "        model_weights = np.array([0.85, 0.15])\n",
        "        print(\"🔧 Male folder detected → Using weights: [0.85, 0.15]\")\n",
        "    elif folder_name == \"female\":\n",
        "        model_weights = np.array([0.15, 0.85])\n",
        "        print(\"🔧 Female folder detected → Using weights: [0.15, 0.85]\")\n",
        "    else:\n",
        "        model_weights = np.array([0.6, 0.4])\n",
        "        print(\"🔧 Custom folder detected → Using weights: [0.6, 0.4]\")\n",
        "    model_weights = model_weights / model_weights.sum()\n",
        "\n",
        "    image_paths = []\n",
        "    subdirs = [p for p in image_folder.iterdir() if p.is_dir()]\n",
        "    if any(subdir.name.lower() in label2idx for subdir in subdirs):\n",
        "        for label in label2idx.keys():\n",
        "            folder = image_folder / label\n",
        "            if folder.exists():\n",
        "                image_paths.extend(folder.glob(\"*\"))\n",
        "    else:\n",
        "        image_paths = list(image_folder.glob(\"*\"))\n",
        "\n",
        "    image_paths = [p for p in image_paths if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
        "    if not image_paths:\n",
        "        raise ValueError(\"No images found!\")\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    display_images = []\n",
        "\n",
        "    for img_path in tqdm(image_paths, desc=\"Running Inference\"):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        face = crop_central_face(img)\n",
        "        tensor = transform(face).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            probs = torch.zeros(1, 2).to(device)\n",
        "            for model, weight in zip(models_list, model_weights):\n",
        "                output = model(tensor)\n",
        "                probs += F.softmax(output, dim=1) * weight\n",
        "\n",
        "            pred = torch.argmax(probs, dim=1).item()\n",
        "            conf = probs[0][pred].item()\n",
        "\n",
        "        predictions.append({\n",
        "            \"filename\": str(img_path.relative_to(image_folder)),\n",
        "            \"predicted_label\": idx2label[pred],\n",
        "            \"confidence\": round(conf, 4)\n",
        "        })\n",
        "        pred_labels.append(pred)\n",
        "\n",
        "        parent = img_path.parent.name.lower()\n",
        "        if parent in label2idx:\n",
        "            true_labels.append(label2idx[parent])\n",
        "        elif folder_name in label2idx:\n",
        "            true_labels.append(label2idx[folder_name])\n",
        "        else:\n",
        "            true_labels.append(None)\n",
        "\n",
        "        resized = face.resize((128, 128))\n",
        "        display_images.append((resized, idx2label[pred], round(conf * 100, 1)))\n",
        "\n",
        "    if len(true_labels) > 0 and all(t is not None for t in true_labels):\n",
        "        print(\"\\n📊 Classification Report:\")\n",
        "        print(classification_report(true_labels, pred_labels, target_names=[\"male\", \"female\"]))\n",
        "        if show_matrix:\n",
        "            cm = confusion_matrix(true_labels, pred_labels)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"male\", \"female\"])\n",
        "            disp.plot(cmap=\"Blues\")\n",
        "            plt.title(\"Confusion Matrix\")\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(\"\\n⚠  No ground truth labels available for performance metrics.\")\n",
        "\n",
        "    if save_csv:\n",
        "        df = pd.DataFrame(predictions)\n",
        "        df.to_csv(\"central_face_ensemble_predictions.csv\", index=False)\n",
        "        print(\"✅ Saved predictions to central_face_ensemble_predictions.csv\")\n",
        "\n",
        "    show_all(display_images)\n",
        "\n",
        "# ---------------- Grid Display ---------------- #\n",
        "def show_all(grid):\n",
        "    cols = 6\n",
        "    rows = (len(grid) + cols - 1) // cols\n",
        "    plt.figure(figsize=(18, 3.5 * rows))\n",
        "    for i, (img, label, conf) in enumerate(grid):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"{label} ({conf}%)\", fontsize=9)\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------------- Entry Point ---------------- #\n",
        "if _name_ == \"_main_\":\n",
        "    model_paths = [\n",
        "        \"/kaggle/input/models-2/best_gender_model best one.pth\",\n",
        "        \"/kaggle/input/testing-m2/best_gender_model ensmeble main.pth\"\n",
        "    ]\n",
        "    model_weights = [0.6, 0.4]  # Default; overridden dynamically\n",
        "    image_folder = \"/kaggle/input/testing1\"\n",
        "    run_ensemble_inference(model_paths, model_weights, image_folder)"
      ],
      "metadata": {
        "id": "qUswqHKGnlPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}