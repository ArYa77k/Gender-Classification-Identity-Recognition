# COMSYS Hackathon-5 (2025) Submission ğŸ”¬ğŸ¤–  
**Team:** _Chehra_  
**Hackathon Theme:** Robust Face Recognition and Gender Classification under Adverse Visual Conditions  
**Organized by:** COMSYS Educational Trust & Warsaw University of Technology  

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.5+-green.svg)](https://opencv.org)
[![FaceNet](https://img.shields.io/badge/FaceNet-PyTorch-lightcoral.svg)](https://github.com/timesler/facenet-pytorch)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-Latest-blue.svg)](https://scikit-learn.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-yellow.svg)](LICENSE)
[![COMSYS](https://img.shields.io/badge/Hackathon-COMSYS%202025-orange.svg)](https://comsysconf.org/2025)
[![Team](https://img.shields.io/badge/Team-Chehra-purple.svg)](https://github.com/yourusername/comsys-hackathon-5)
---

## âš ï¸ Important Notes Before Use

> **ğŸ“Œ PLEASE READ CAREFULLY**

- For **Task A**, the official documentation clearly required:  
  _"Precision, Recall, Accuracy, F1-score"_ as evaluation metrics.  
  âœ… Our models were designed and evaluated with these metrics.

- For **Task B**, only _Top-1 Accuracy_ and _Macro-Averaged F1 Score_ were mentioned in the brief.  
  â¤ Therefore, we did **not compute Precision/Recall during training.**

- However, since the final **Google Form** requested **Precision, Recall, Accuracy, Loss** for Task B as well,  
  ğŸ” we re-analyzed the model outputs and computed **approximate estimates** for these using classification outputs post-training.  
  ğŸ§  The logic and calculations for this are explained in the codebase and technical documentation.

- âš ï¸ For Task B: The training script mistakenly attempts validation on the training set assuming folder overlap.  
  The **Train and Validation sets are non-overlapping** â†’ this results in **zero accuracy**.  
  âœ… Please ignore this validation logic and use the **separate provided Task B validation script**.

---

## Directory Structure

```
CHEHRA/
â”‚
â”œâ”€â”€ Task_A/                                    # Gender Classification Task
â”‚   â”œâ”€â”€ resnet_18_M1_model_with_weights.pt     # Model 1 with weights
â”‚   â”œâ”€â”€ resnet18_M1_state_dictionary.pth       # Model 1 state dictionary
â”‚   â”œâ”€â”€ resnet_18_M2_model_with_weights.pt     # Model 2 with weights
â”‚   â”œâ”€â”€ resnet18_M2_state_dictionary.pth       # Model 2 state dictionary
â”‚   â””â”€â”€ Team_Chehra_Task_A (1).ipynb          # Task A notebook
â”‚
â”œâ”€â”€ Task_B/                                    # Face Recognition Task
â”‚   â”œâ”€â”€ best_face_encoder (2).pth              # Best face encoder model
â”‚   â”œâ”€â”€ COMSYS_Task_B_Identity_Recognition_Summary.docx # Task B documentation
â”‚   â””â”€â”€ Team_Chehra_Submission_B (1).ipynb    # Task B notebook
â”‚
â”œâ”€â”€ LICENSE                                    # License file
â””â”€â”€ README.md                                  # Project documentation

---


**Please Take a look at the Requirements.txt and install all required libraries and modules before running the code**

## ğŸ§  Task A â€“ Gender Classification (Binary)

### ğŸ§ª Model 1: Augmentation-Based ResNet18
- âœ… Trained on full images with extensive data augmentation.
- ğŸ“Š Metrics: Accuracy, Precision, Recall, F1-score
- ğŸ“ Scripts:
  - `train_model_1.ipynb` â€” Train using original dataset
  - `validate_model_1.ipynb` â€” Use datasetâ€™s validation folder directly (structure: `Task_A/val/male`, `Task_A/val/female`)
  - ğŸ”„ Uses full image input and balanced training.

### ğŸ§ª Model 2: Cropped Face Central Ensemble (MT-CNN + ResNet18)
- ğŸ§  Uses `facenet-pytorch`â€™s MT-CNN to crop central face
- ğŸ§¬ Ensemble of:
  - One model trained on original data
  - One model trained on MT-CNN cropped data
- ğŸ“ Scripts:
  - `train_model_2.ipynb`
  - `validate_model_2.ipynb` â€” same structure as Model 1
  - `central_crop_ensemble_inference.ipynb` â€” for **custom** folders
    - Detects folder name suffix:
      - `"male"` â†’ weights `[0.85, 0.15]`
      - `"female"` â†’ weights `[0.15, 0.85]`
      - Otherwise â†’ `[0.6, 0.4]`

---

## ğŸ§  Task B â€“ Face Recognition (Multi-Class)

### ğŸ§¬ Model: Triplet-Loss Based Face Encoder (ResNet18)
- Lightweight, stable ResNet18 backbone for small dataset
- Triplet loss minimizes intra-class distance and maximizes inter-class separation
- ğŸ“ Scripts:
  - `train_face_encoder.ipynb` â€” Model trained with Triplet Loss
    - **Note**: In-built validation script will **fail** (Train â‰  Val identities).
  - `validate_face_encoder.ipynb` â€” Use this with proper validation folder
  - `inference_face_encoder.ipynb` â€” Custom inference with support for:
    - Query-Gallery matching
    - Visual similarity outputs
    - Cosine-based Top-1 prediction

### ğŸ§  Evaluation Metrics:
- Top-1 Accuracy (on validation): **99.26%**
- Estimated Training Metrics:
  - Accuracy: ~**99.8â€“100%**
  - F1 Score: ~**99.8â€“100%**
  - Precision / Recall: ~**99.8â€“100%**

---

## ğŸš€ Usage Instructions (for Kaggle / Local)

1. **Open Kaggle Notebook**
2. **Upload the repo**
3. **Upload dataset into `/kaggle/input/`**
4. Run any of the following:
   - Task A:
     - `validate_model_1.ipynb` â€” standard validation on dataset folders
     - `validate_model_2.ipynb` â€” same but for cropped-face model
     - `central_crop_ensemble_inference.ipynb` â€” **for any custom folder** (flat or labeled)
   - Task B:
     - `validate_face_encoder.ipynb` â€” for known gallery-query validation
     - `inference_face_encoder.ipynb` â€” for **custom matching**

> âš™ï¸ To customize paths: **Edit only `folder_path` or `model_path`** in respective cells.

> âš ï¸ DO NOT change the training or backbone architecture unless retraining entirely.

---

## âœ¨ Final Notes

- Feel free to tweak hyperparameters and folder names if needed.
- All models and scripts are cleanly separated for flexibility.
- This solution was optimized for robustness under **adverse visual conditions**, generalization, and ease of inference.

---

## ğŸ‘¨â€ğŸ”¬ Contact
If you'd like to reach out for queries or improvements:
- Name: Md Haaris Hussain
- Email: mdhaarishussain@gmail.com

---

**Good luck to everyone! May the best model win ğŸ†**
